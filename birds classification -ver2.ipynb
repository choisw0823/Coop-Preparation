{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5468571,"sourceType":"datasetVersion","datasetId":534640},{"sourceId":993,"sourceType":"modelInstanceVersion","modelInstanceId":847}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pylab as plt\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow_hub as hub\n\nimport os\nimport pandas as pd\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, Callback\n\n\n# Set memory growth for GPU devices\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n\nprint(\"TF version:\", tf.__version__)\nprint(\"Hub version:\", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-23T08:43:49.013271Z","iopub.execute_input":"2024-01-23T08:43:49.013698Z","iopub.status.idle":"2024-01-23T08:43:49.021960Z","shell.execute_reply.started":"2024-01-23T08:43:49.013663Z","shell.execute_reply":"2024-01-23T08:43:49.021058Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"TF version: 2.13.0\nHub version: 0.14.0\nGPU is available\n","output_type":"stream"}]},{"cell_type":"code","source":"train_directory='../input/100-bird-species/train'\nval_directory='../input/100-bird-species/valid'\ntest_directory='../input/100-bird-species/test'\n\n\ntrain_datagen=ImageDataGenerator(\n rescale=1/255.0, \n rotation_range=10, \n zoom_range=0.05, \n width_shift_range=0.05, \n height_shift_range=0.05, \n shear_range=0.05,\n horizontal_flip=True,\n fill_mode='nearest')\n\nbatch_size =64\n\ntrain_generator = train_datagen.flow_from_directory(\n    directory=train_directory,\n    target_size=(224,224),\n    color_mode='rgb',\n    batch_size=batch_size,\n    class_mode='categorical', \n    shuffle=True, \n    seed=42\n)\nval_datagen=ImageDataGenerator(rescale=1/255.0)\nvalid_generator = val_datagen.flow_from_directory(\n    directory=val_directory,\n    target_size=(224, 224),\n    color_mode=\"rgb\",\n    batch_size=batch_size,\n    class_mode=\"categorical\",\n    shuffle=True,\n    seed=42\n)\ntest_datagen = ImageDataGenerator(rescale=1/255.0) \ntest_generator = test_datagen.flow_from_directory(\n  directory=test_directory,\n  target_size=(224,224),\n    color_mode='rgb',\n    batch_size=batch_size,\n    class_mode=\"categorical\",\n    shuffle=False,\n    seed=42\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T08:43:49.023774Z","iopub.execute_input":"2024-01-23T08:43:49.024669Z","iopub.status.idle":"2024-01-23T08:43:55.570275Z","shell.execute_reply.started":"2024-01-23T08:43:49.024635Z","shell.execute_reply":"2024-01-23T08:43:55.569404Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Found 84635 images belonging to 525 classes.\nFound 2625 images belonging to 525 classes.\nFound 2625 images belonging to 525 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Training\nsteps_per_epoch=train_generator.n//train_generator.batch_size\nvalidation_steps=valid_generator.n//valid_generator.batch_size\ntest_steps=test_generator.n//test_generator.batch_size\nepochs = 50\nlr = 0.0001\n\n# Create the model\nmodel = tf.keras.Sequential([\n    # Pre-trained ResNet-50 model\n    tf.keras.Input(shape=(224,224,3)),\n    hub.KerasLayer(\"https://www.kaggle.com/models/tensorflow/resnet-50/frameworks/TensorFlow2/variations/feature-vector/versions/1\", trainable=False),\n    tf.keras.layers.Flatten(),\n\n    # Additional layers\n    # tf.keras.layers.Dropout(rate=0.2),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    # tf.keras.layers.Dropout(rate=0.2),\n    # tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(rate=0.3),\n    # Output Dense layer\n    tf.keras.layers.Dense(525,  activation= 'softmax')\n])\n\n\n\n# model.build([None, 224, 224, 3])\n# model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=lr),metrics=['acc'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-23T08:43:55.572218Z","iopub.execute_input":"2024-01-23T08:43:55.572611Z","iopub.status.idle":"2024-01-23T08:44:03.633909Z","shell.execute_reply.started":"2024-01-23T08:43:55.572578Z","shell.execute_reply":"2024-01-23T08:44:03.633040Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Attaching model 'tensorflow/resnet-50/tensorflow2/feature-vector/1' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"Model: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n keras_layer_7 (KerasLayer)  (None, 2048)              23561152  \n                                                                 \n flatten_7 (Flatten)         (None, 2048)              0         \n                                                                 \n dense_14 (Dense)            (None, 1024)              2098176   \n                                                                 \n dropout_7 (Dropout)         (None, 1024)              0         \n                                                                 \n dense_15 (Dense)            (None, 525)               538125    \n                                                                 \n=================================================================\nTotal params: 26197453 (99.94 MB)\nTrainable params: 2636301 (10.06 MB)\nNon-trainable params: 23561152 (89.88 MB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Instantiate an optimizer.\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n# Instantiate a loss function.\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n# Instantiate a metric to track accuracy\naccuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n\n# Instantiate early stopping and model checkpoint parameters\nearly_stopping = {'patience': 3, 'best_loss': float(\"inf\")}\nmodel_checkpoint = {'filepath': '/kaggle/working/best_model.h5'}\n\n\nfor epoch in range(epochs):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n    # Reset the accuracy at the start of each epoch\n    accuracy_metric.reset_states()\n    \n    # Iterate over the batches of the dataset.\n    for step, (x_batch_train, y_batch_train) in enumerate(train_generator):\n        # Open a GradientTape to record the operations run\n        # during the forward pass, which enables auto-differentiation.\n        with tf.GradientTape() as tape:\n            # Run the forward pass of the layer.\n            # The operations that the layer applies\n            # to its inputs are going to be recorded\n            # on the GradientTape.\n            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n\n            # Compute the loss value for this minibatch.\n            loss_value = loss_fn(y_batch_train, logits)\n\n        # Use the gradient tape to automatically retrieve\n        # the gradients of the trainable variables with respect to the loss.\n        grads = tape.gradient(loss_value, model.trainable_weights)\n\n        # Run one step of gradient descent by updating\n        # the value of the variables to minimize the loss.\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n        \n        # Update the accuracy metric\n        accuracy_metric.update_state(y_batch_train, logits)\n        # Log every 200 batches.\n        if step % 200 == 0:\n            print(\n                \"Training loss (for one batch) at step %d: %.4f\"\n                % (step, float(loss_value))\n            )\n            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n        if step >= steps_per_epoch:\n            break\n    # Calculate and print the accuracy at the end of each epoch\n    train_accuracy = accuracy_metric.result().numpy()\n            \n    print(\"Training accuracy at the end of epoch %d: %.4f\" % (epoch, train_accuracy))\n    \n    # Assuming you have validation data\n    # Evaluate on the validation set without using model.evaluate\n    accuracy_metric.reset_states()\n    validation_loss = 0.0  # Initialize validation loss\n\n    for step, (x_batch_train, y_batch_train) in enumerate(valid_generator):\n        val_logits = model(x_batch_val, training=False)\n        val_loss = loss_fn(y_batch_val, val_logits)\n\n        # Accumulate the validation loss\n        validation_loss += val_loss.numpy()\n\n        accuracy_metric.update_state(y_batch_val, val_logits)\n        if step >= validation_steps:\n            break\n\n    # Compute the mean validation loss\n    validation_loss /= len(valid_generator)\n\n    validation_accuracy = accuracy_metric.result().numpy()\n\n    print(\"Validation loss: %.4f\" % validation_loss)\n    print(\"Validation accuracy: %.4f\" % validation_accuracy)\n    \n\n    # Assuming you have validation data\n    # Evaluate on the validation set without using model.evaluate\n    accuracy_metric.reset_states()\n    test_loss = 0.0  # Initialize validation loss\n\n    for step, (x_batch_train, y_batch_train) in enumerate(test_generator):\n        test_logits = model(x_batch_val, training=False)\n        test_loss = loss_fn(y_batch_val, val_logits)\n\n        # Accumulate the validation loss\n        test_loss += test_loss.numpy()\n\n        accuracy_metric.update_state(y_batch_val, val_logits)\n        if step >= test_steps:\n            break\n\n    # Compute the mean validation loss\n    test_loss /= len(test_generator)\n\n    test_accuracy = accuracy_metric.result().numpy()\n    \n    print(\"Test loss: %.4f\" % test_loss)\n    print(\"Test accuracy: %.4f\" % test_accuracy)\n    \n\n    # Check if the validation loss has improved\n    if validation_loss < early_stopping['best_loss']:\n        early_stopping['best_loss'] = validation_loss\n        early_stopping['wait'] = 0  # Reset the patience counter\n        model.save_weights(model_checkpoint['filepath'])\n        print(\"Saved model weights to '%s'\" % model_checkpoint['filepath'])\n    else:\n        early_stopping['wait'] += 1\n        print(\"No improvement in validation loss. Patience: %d/%d\" % (early_stopping['wait'], early_stopping['patience']))\n\n    # Check if early stopping criteria are met\n    if early_stopping['wait'] >= early_stopping['patience']:\n        print(\"Early stopping criterion reached. Stopping training.\")\n        break\n\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T08:44:03.635142Z","iopub.execute_input":"2024-01-23T08:44:03.635420Z","iopub.status.idle":"2024-01-23T14:31:44.353069Z","shell.execute_reply.started":"2024-01-23T08:44:03.635394Z","shell.execute_reply":"2024-01-23T14:31:44.352036Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"\nStart of epoch 0\nTraining loss (for one batch) at step 0: 6.5712\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 4.6014\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 2.9377\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 2.2987\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 1.7580\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 1.5953\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 1.1409\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 0: 0.4584\nValidation loss: 0.9204\nValidation accuracy: 0.8125\nTest loss: 0.0438\nTest accuracy: 0.8125\nSaved model weights to '/kaggle/working/best_model.h5'\n\nStart of epoch 1\nTraining loss (for one batch) at step 0: 1.2060\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 1.0941\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.9448\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 1.0191\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 0.8256\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 0.6976\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 0.7954\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 1: 0.7637\nValidation loss: 0.5379\nValidation accuracy: 0.8906\nTest loss: 0.0256\nTest accuracy: 0.8906\nSaved model weights to '/kaggle/working/best_model.h5'\n\nStart of epoch 2\nTraining loss (for one batch) at step 0: 0.7863\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 0.9377\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.6104\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 0.7201\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 0.4581\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 0.4654\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 0.6184\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 2: 0.8303\nValidation loss: 0.4508\nValidation accuracy: 0.9219\nTest loss: 0.0215\nTest accuracy: 0.9219\nSaved model weights to '/kaggle/working/best_model.h5'\n\nStart of epoch 3\nTraining loss (for one batch) at step 0: 0.7139\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 0.4160\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.5241\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 0.6494\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 0.4939\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 0.5096\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 0.4503\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 3: 0.8638\nValidation loss: 0.3410\nValidation accuracy: 0.9219\nTest loss: 0.0162\nTest accuracy: 0.9219\nSaved model weights to '/kaggle/working/best_model.h5'\n\nStart of epoch 4\nTraining loss (for one batch) at step 0: 0.6261\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 0.7401\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.3763\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 0.2379\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 0.6676\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 0.3768\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 0.4207\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 4: 0.8859\nValidation loss: 0.2774\nValidation accuracy: 0.9531\nTest loss: 0.0132\nTest accuracy: 0.9531\nSaved model weights to '/kaggle/working/best_model.h5'\n\nStart of epoch 5\nTraining loss (for one batch) at step 0: 0.4442\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 0.2740\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.3017\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 0.3187\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 0.2892\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 0.4564\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 0.4005\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 5: 0.9015\nValidation loss: 0.2967\nValidation accuracy: 0.9375\nTest loss: 0.0141\nTest accuracy: 0.9375\nNo improvement in validation loss. Patience: 1/3\n\nStart of epoch 6\nTraining loss (for one batch) at step 0: 0.2216\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 0.3291\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.2332\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 0.2375\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 0.3304\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 0.2816\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 0.2200\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 6: 0.9142\nValidation loss: 0.2276\nValidation accuracy: 0.9531\nTest loss: 0.0108\nTest accuracy: 0.9531\nSaved model weights to '/kaggle/working/best_model.h5'\n\nStart of epoch 7\nTraining loss (for one batch) at step 0: 0.2842\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 0.1414\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.3811\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 0.2655\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 0.1357\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 0.2808\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 0.3272\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 7: 0.9224\nValidation loss: 0.2317\nValidation accuracy: 0.9531\nTest loss: 0.0110\nTest accuracy: 0.9531\nNo improvement in validation loss. Patience: 1/3\n\nStart of epoch 8\nTraining loss (for one batch) at step 0: 0.2604\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 0.1349\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.2233\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 0.3206\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 0.4100\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 0.3155\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 0.0984\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 8: 0.9321\nValidation loss: 0.2536\nValidation accuracy: 0.9375\nTest loss: 0.0121\nTest accuracy: 0.9375\nNo improvement in validation loss. Patience: 2/3\n\nStart of epoch 9\nTraining loss (for one batch) at step 0: 0.2200\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 0.1994\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.2922\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 0.3619\nSeen so far: 38464 samples\nTraining loss (for one batch) at step 800: 0.1853\nSeen so far: 51264 samples\nTraining loss (for one batch) at step 1000: 0.1472\nSeen so far: 64064 samples\nTraining loss (for one batch) at step 1200: 0.2108\nSeen so far: 76864 samples\nTraining accuracy at the end of epoch 9: 0.9369\nValidation loss: 0.2750\nValidation accuracy: 0.9219\nTest loss: 0.0131\nTest accuracy: 0.9219\nNo improvement in validation loss. Patience: 3/3\nEarly stopping criterion reached. Stopping training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Define callbacks\n\n\n# class CustomCallback(Callback):\n#     def on_epoch_end(self, epoch, logs=None):\n#         if (epoch + 1) % 5 == 0:\n#             val_acc = self.model.evaluate(test_generator, return_dict=True)['acc']\n#             print(f'test accuracy at epoch {epoch + 1}: {val_acc}')\n\n#             self.model.save(f'/kaggle/working/models/model_val_acc_{epoch + 1}.h5')\n\n# custom_callback = CustomCallback()\n\n\n\n\n# model.fit(\n#     train_generator,\n#     epochs=epochs,\n#     steps_per_epoch=steps_per_epoch,\n#     validation_data=valid_generator,\n#     validation_steps=validation_steps,\n#     callbacks=[tensorboard_callback, custom_callback]\n# )\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T14:31:44.355550Z","iopub.execute_input":"2024-01-23T14:31:44.356194Z","iopub.status.idle":"2024-01-23T14:31:44.360644Z","shell.execute_reply.started":"2024-01-23T14:31:44.356155Z","shell.execute_reply":"2024-01-23T14:31:44.359708Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# results = model.evaluate(test_generator, verbose=0)\n\n# print(\"    Test Loss: {:.5f}\".format(results[0]))\n# print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2024-01-23T14:31:44.361718Z","iopub.execute_input":"2024-01-23T14:31:44.361986Z","iopub.status.idle":"2024-01-23T14:31:44.396271Z","shell.execute_reply.started":"2024-01-23T14:31:44.361962Z","shell.execute_reply":"2024-01-23T14:31:44.395408Z"},"trusted":true},"execution_count":35,"outputs":[]}]}